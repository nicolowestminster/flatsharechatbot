# -*- coding: utf-8 -*-
"""secondtrialflatshare.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1psDhnVFcD-D2TqSFARWvZHTTiL5Sg-sO
"""

!pip install openai==0.28

# On the above section openai 0.28 has been used as at the moment of the creation of the project this was the available version of the package that matched with the criteria of the code sections
# On the below section 2 packages have been installed, numpy and tiktoken (which is a package that allows for token counts)

!pip install numpy
!pip install tiktoken

import openai
import csv
import json
import os
import numpy as np
from collections import defaultdict
import tiktoken

openai.api_key = " ADD YOUR KEY HERE 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"

import csv
import json

csv_file_path = '/content/drive/MyDrive/secondtrialflatshare/flatsharedatasetUTF8.csv'
jsonl_file_path = '/content/drive/MyDrive/secondtrialflatshare/flatsharedatasetUTF8-json.jsonl'

# Function that allows the cleaning and adjustment to be made to JSON from the cells
def decode_json(cell):
    try:
        # transformation of the format to a JSON Object for compatibility
        return json.loads(cell)
    except json.JSONDecodeError as e:
        print(f"JSON decode error for cell '{cell}': {e}")
        return None

# Code to process every row of the CSV file
cleaned_data = []
with open(csv_file_path, 'r', encoding='utf-8-sig') as file:
    csv_reader = csv.reader(file)
    for row in csv_reader:
        # Assuming the first cell of each row is the JSON data
        cell_json = decode_json(row[0])
        if cell_json:
            cleaned_data.append(cell_json)

# Write data to a JSON Lines file
with open(jsonl_file_path, 'w', encoding='utf-8') as jsonl_file:
    for item in cleaned_data:
        jsonl_file.write(json.dumps(item) + '\n')

#from OpenAI website to format data;  https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset

# here on the data_path section is indicated where the JSON Lines file is located, in this case in the google drive

data_path = '/content/drive/MyDrive/secondtrialflatshare/flatsharedatasetUTF8-json.jsonl'

# Load dataset reading the JSON Lines file
with open(data_path) as f:
    dataset = [json.loads(line) for line in f]


# Initial dataset stats
print("Num examples:", len(dataset))
print("First example:")
for message in dataset[0]["messages"]:
    print(message)


# Format error checks
format_errors = defaultdict(int)

for ex in dataset:
    if not isinstance(ex, dict):
        format_errors["data_type"] += 1
        continue

    messages = ex.get("messages", None)
    if not messages:
        format_errors["missing_messages_list"] += 1
        continue

    for message in messages:
        if "role" not in message or "content" not in message:
            format_errors["message_missing_key"] += 1

        if any(k not in ("role", "content", "name") for k in message):
            format_errors["message_unrecognized_key"] += 1

        if message.get("role", None) not in ("system", "user", "assistant"):
            format_errors["unrecognized_role"] += 1

        content = message.get("content", None)
        if not content or not isinstance(content, str):
            format_errors["missing_content"] += 1

    if not any(message.get("role", None) == "assistant" for message in messages):
        format_errors["example_missing_assistant_message"] += 1

if format_errors:
    print("Found errors:")
    for k, v in format_errors.items():
        print(f"{k}: {v}")
else:
    print("No errors found")

# Beyond the structure of the message, we also need to ensure that the length does not exceed the 4096 token limit.

# Tiktoken comes into function which will allow for token count
encoding = tiktoken.get_encoding("cl100k_base")


# following code is from the openAI cookbook https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb
def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):
    num_tokens = 0
    for message in messages:
        num_tokens += tokens_per_message
        for key, value in message.items():
            num_tokens += len(encoding.encode(value))
            if key == "name":
                num_tokens += tokens_per_name
    num_tokens += 3
    return num_tokens

def num_assistant_tokens_from_messages(messages):
    num_tokens = 0
    for message in messages:
        if message["role"] == "assistant":
            num_tokens += len(encoding.encode(message["content"]))
    return num_tokens

def print_distribution(values, name):
    print(f"\n#### Distribution of {name}:")
    print(f"min / max: {min(values)}, {max(values)}")
    print(f"mean / median: {np.mean(values)}, {np.median(values)}")
    print(f"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}")

# process the creation of the fine-tuning job:

# Warnings and tokens counts
n_missing_system = 0
n_missing_user = 0
n_messages = []
convo_lens = []
assistant_message_lens = []

for ex in dataset:
    messages = ex["messages"]
    if not any(message["role"] == "system" for message in messages):
        n_missing_system += 1
    if not any(message["role"] == "user" for message in messages):
        n_missing_user += 1
    n_messages.append(len(messages))
    convo_lens.append(num_tokens_from_messages(messages))
    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))

print("Num examples missing system message:", n_missing_system)
print("Num examples missing user message:", n_missing_user)
print_distribution(n_messages, "num_messages_per_example")
print_distribution(convo_lens, "num_total_tokens_per_example")
print_distribution(assistant_message_lens, "num_assistant_tokens_per_example")
n_too_long = sum(l > 4096 for l in convo_lens)
print(f"\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning")

# Pricing and default n_epochs estimate
MAX_TOKENS_PER_EXAMPLE = 4096

MIN_TARGET_EXAMPLES = 100
MAX_TARGET_EXAMPLES = 25000
TARGET_EPOCHS = 3
MIN_EPOCHS = 1
MAX_EPOCHS = 25

n_epochs = TARGET_EPOCHS
n_train_examples = len(dataset)
if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:
    n_epochs = min(MAX_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)
elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:
    n_epochs = max(MIN_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)

n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)
print(f"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training")
print(f"By default, you'll train for {n_epochs} epochs on this dataset")
print(f"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens")

# The next lines of code allows for an estimated cost per 100,000 tokens
cost_per_100k_tokens = 0.80
estimated_cost = ((n_epochs * n_billing_tokens_in_dataset) / 100000) * cost_per_100k_tokens
print(f"Estimated cost for fine-tuning: approximately ${estimated_cost:.2f}") #I added this for actual cost based on current pricing

# Function to save the dataset as a JSONL file
def save_to_jsonl(conversations, file_path):
    with open(file_path, 'w') as file:
        for conversation in conversations:
            json_line = json.dumps(conversation)
            file.write(json_line + '\n')

# Specify the path where you want to save the JSONL file in your Google Drive
jsonl_file_path = '/content/drive/MyDrive/secondtrialflatshare/flatsharedatasetUTF8-json-clean.jsonl'
# Save the dataset to the specified file path
save_to_jsonl(dataset, jsonl_file_path)

#Upload data for training
training_file_name = '/content/drive/MyDrive/secondtrialflatshare/flatsharedatasetUTF8-json-clean.jsonl'

training_response = openai.File.create(
    file=open(training_file_name, "rb"), purpose="fine-tune"
)
training_file_id = training_response["id"]

#Gives training file id
print("Training file id:", training_file_id)

#Create Fine-Tuning Job
suffix_name = "Nicolo-the-Agent"

response = openai.FineTuningJob.create(
    training_file=training_file_id,
    model="gpt-3.5-turbo",
    suffix=suffix_name,
)

job_id = response["id"]

print(response)

#list events as fine-tuning progresses
response = openai.FineTuningJob.list_events(id=job_id, limit=50)

events = response["data"]
events.reverse()

for event in events:
    print(event["message"])

#retrieve fine-tune model id
response = openai.FineTuningJob.retrieve(job_id)
fine_tuned_model_id = response["fine_tuned_model"]

print(response)
print("\nFine-tuned model id:", fine_tuned_model_id)

#Test it out!
test_messages = []

system_message = "You are Nicolo is a Flatshare estate agent that is helpful and will do a quick pre-qualification check and collect some information from the client. How it works? It works that the applicant looking for a room send a message to the chatbot which handles the initial conversation that an agent needs to do in order to understand if the applicant can pass the reference check for the room. Basically the bot will follow a sequence of questions ('What is the price of the room?' collects the information and then asks 'Are you working or you are a student?' and then asks 'What is your salary?' if it is a professional, otherwise if it is a student it will ask if the applicant has a guarantor. Then the bot will make a mathematical operation which is to multiply the monthly cost of the room 30 times and if the yearly salary of the applicant is above the threshold will congratulate and say 'One of the agents will be in touch with you shortly' otherwise it will ask if the applicant can pay upfront and if the applicant is not able, then the chatbot will say 'I'm sorry but at this stage we can't proceed with your application'). The chatbot will then send a confirmation with the list of suitable applicants for a specific price of accomodation."
test_messages.append({"role": "system", "content": system_message})
user_message = "The cost of the room is 900 per month"
test_messages.append({"role": "user", "content": user_message})

print(test_messages)

#OpenAI Chat Completions
response = openai.ChatCompletion.create(
    model=fine_tuned_model_id, #can test it against gpt-3.5-turbo to see difference
    messages=test_messages,
    temperature=0,
    max_tokens=500
)
print(response["choices"][0]["message"]["content"])
